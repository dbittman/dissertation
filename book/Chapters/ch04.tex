
\chapter{The Data-Centric Approach}\label{ch:datacentric}

\squo{It's the wanting to know that makes us matter.}{\emph{Arcadia}, Tom Stoppard}

\begin{chabstract}
    The last few chapters discussed the hardware trends and the modern needs of software. This chapter will take those
    insights and define a design space within which we will build a data-centric OS. We will then conclude with a look
    back towards related, prior work.
\end{chabstract}

\section{An Opportunity Arises}


The confluence of hardware trends (Chapter~\ref{ch:farout}) and software needs (Chapter~\ref{ch:softwaredemands})
demands a fundamental shift in the way we program computers. Our two-tier memory hierarchy does not adequately reflect
what hardware is trending to provide nor does it provide adequate abstractions for applications to operate on data with
low overhead. Meanwhile, the increasing heterogeneity of our computing environments---different kinds of memory
at different distances, specialized computing devices, near-storage compute, and remote computation---force us into a
corner when trying to use current operating system abstractions to model what is, in essence, a global environment of computation and
data. It is vital that operating systems evolve to provide new abstractions---they must make the
best possible use of hardware trends to provide a programming model that aligns with software's demands for low-overhead
computation. Mere incremental change is insufficient for enabling dramatic improvements in performance and complexity.

Presented with these trends, we are building a \emph{data-centric} operating system, one that eschews the traditional
process-centric (or actor-centric) model of operating system design in favor of making \emph{data} the primary citizen.
In process-centric models, the application is the first-class citizen, operating on data in isolation, performing
explicit persistence and network activity, and relying heavily on serialization. Instead, as we will see, a focus on data in a global
space makes it possible to enable better sharing (both between applications and devices on one machine and between
nodes), lower overhead computation, and simpler applications.

\squo{Clearly, for
    computer systems to be interesting, both input and output are required.}{Operating Systems: Three Easy Pieces~\cite{ostep}}
Fundamentally, data is and has always been the focus of programming. In the past, our abstractions have failed to
adequately capture this simple fact. But now, as software looks towards more modern programming models and the friction
between hardware and current models grows, we have an opportunity to reimagine how we structure programs and data.
It is a convenient time to make a break, when the impedance is low---software will need to change to take advantage of
new hardware, and we should support those applications with better fitting programming models.


% TODO
%Providing a model that focuses on data sharing, concurrency, and fine-grained persistence does not automatically make
%things easier---while some of our work, as we will see, does alleviate some traditionally difficult aspects, those hard
%things will still be hard.

%If we are to provide a clean-break model built around current hardware and software demands, we will need to provide a
%compelling reason for developers to invest in switching over. But with this confluence of hardware and software trends,
%the impedance to adopt a new model is at an all time low.


\section{A Design Space}

The idea of a data-centric operating system in which in-memory data is the primary citizen still leaves a large space for how
we might actually design such a system. Indeed, the remainder of this dissertation will be focused mostly on our design and
implementation choices, the rationale for those choices, and empirical evaluation. However, it is still worth taking a
minute to discuss the design space we are in and nail down what it means to design a data-centric OS that meets the
requirements set out in the previous chapters.

\paragraph{Low Kernel Involvement} As we saw earlier, the kernel imposes a heavy overhead on applications that wish to
access data outside their local context, even in the case of interfaces like \texttt{mmap}, particularly as device
latency continues to drop. A data-centric OS should, therefore, avoid requiring significant kernel intervention, instead
preferring to allow applications to access data directly as in-memory data structures\sidenote{See below.}. The reduction in
kernel involvement and responsibility has some wide-reaching effects, which we call ``the death of the process''.

Processes as a first class OS abstraction are, like virtual addresses, unnecessary; a traditional process couples
threads of control to a virtual address space, a security role, and kernel state.
%an environment for a set of threads to operate in.
However, with the kernel removed from
persistent data access, much of that kernel state\sidenote{\Eg file descriptors, file abstractions, \etc.} is unnecessary,
leading to a decoupling of mechanisms:
nothing fundamentally connects a virtual address space
(a piece of ephemeral context used to access data) and a security context (\emph{what} data
threads may access).
Instead, a data-centric OS can keep the good parts of a process but \emph{separate} virtual address
translation and security roles, allowing threads to select one of each as needed.

\iffalse
    \sidenote{Of course, we do not want to throw out \emph{good} parts of a process. Mechanisms like
        protection, shared memory, and (in some cases) hierarchical responsibility still have their uses,
        but do not need to be coalesced into a single, rigid abstraction. Their separation would have
        significant effects on the POSIX model, but our model, built from clean abstractions,
        can make use of gained flexibility\sidenote{As we will see in Section~\ref{sec:eval}.}.}.
\fi

The process abstraction is just one example. Persistent data access and sharing plays a key role in OS
abstraction design, and we need to avoid complexity arising from combining old and new interfaces.
Hence, we need to consider the
wide-reaching effects of changing the persistence model on \emph{all} aspects of the system, not
just I/O interfaces. These hardware trends give us an opportunity to design an OS around the requirements
of the target programming model instead of trying to mold support libraries around existing
interfaces. While it is important that we provide support for legacy applications,
it is these applications that should be relegated to support libraries; new applications built for
the programming model should get first-class OS support.


\paragraph{In-memory Data Structures and Global Addressing}
The need to place in-memory data structures front and center comes from both the desire to reduce serialization
overhead (Chapter~\ref{ch:softwaredemands}) but also to allow better sharing and direct use of distributed or persistent
memory (Chapter~\ref{ch:farout}). As we discussed, the virtual address space does \emph{not} work for this
purpose\sidenote{We will discuss single address space operating systems later in this chapter.} as it is ephemeral.
Furthermore, sharing data with not just other applications but other nodes and devices demands a large and global
address space of data to avoid the context problem. But
we do still need a mechanism to allow the organization of data at a large scale, the coordination of that address space,
and a method for naming data within a global address space that is invariant of local context. The solution to global
addressing and data references makes up the bulk of the work presented here, but the result is a system that can share
data with a 100\% byte-level copy across nodes and between processes where the pointers \emph{all mean the same thing}.


\paragraph{Pointers as a Common Language} Data references are a fundamental part of encoding data relationships\sidenote{See
    Chapter~\ref{ch:softwaredemands}.}. When we consider the implication of encoding data references within a truly global
address space, we find that we can treat references as a \emph{common language}. If all nodes, processes, and devices
agree not only on names for data but also \emph{how} to name data, they can more easily share data and interoperate. In
fact, levels of the infrastructure that previously had no view into data relationships can now gain additional insight
when performing tasks.

This addresses the issues with RPC that we discussed in Chapter~\ref{ch:softwaredemands}.
We can combine the code mobility of RPC with the flexibility offered by
DSM-like models in a global address space with invariant references as a first-class abstraction. By
imbuing data with fundamental identity and pushing an understanding of data references into the OS
and the network, we can leverage aspects of content-based networks to reduce the coordination
typically required in a shared, distributed address space. The programmer is then free to express
their computation through references to code to run on some \emph{references} to data, instead
of needing to serialize and copy \emph{values} for arguments.
Today, developers are often forced to implement functionality such as caching, prefetching, and
manual data movement in preparation for some operation.
With data references as a common
language between the OS, the network, and applications, we can move this infrastructure-level functionality
out of the application and back \emph{into the infrastructure} where it belongs.

\paragraph{Persistence is Sharing} In Chapter~\ref{ch:farout} we discussed how persistence is getting closer to compute
in multiple ways. A natural approach would be to focus on \emph{orthogonal persistence}, in which applications write
data and it is automatically persisted. However, mere orthogonal persistence misses out on a more fundamental
relationship with distribution of memory and applications. Our view is that persistence and distribution are
two sides of the same coin---we saw previously that both are plagued by the same problems of overhead, and both domains
are seeing dramatic improvements in performance. In a data-centric OS, persistence \emph{is} sharing, and the
problems of persistence\sidenote{Including, but not limited to, failure-atomicity and durability.} can be solved in terms of atomicity and transactions\sidenote{As
    we will discuss in Section~\ref{sec:crash}.}.

\section{Twizzler: A Point in the Design Space}

The consequences of meeting the requirements of these hardware and software trends
define a bounded design space for data-centric OSes that we layed out above. We have
chosen a point in that space and built \Twizzler, our approach to providing
applications with efficient and effective access to new, ``far out'' memory hierarchies.
The core philosophy of \Twizzler is that \textbf{any context necessary to interpret some data must be stored with that
    data}.
In the
following chapters we will discuss our design choices, including our global address space, invariant pointers, and
Twizzler OS services and libraries, and see how this philosophy, when combined with the ideas behind a data-centric OS,
enable us to build an efficient implementation of a modern approach to programming model design.

\Twizzler is a stand-alone kernel and userspace runtime that provides execution support
for programs. It provides, as first-class abstractions, a notion of threads, address spaces,
persistent objects, and security contexts. A program
typically executes as a number of threads in a single address space, providing backwards
compatibility with existing programming models, into which persistent objects are mapped on-demand.
Instead of providing a process abstraction, \Twizzler provides \emph{views}
(Sections~\ref{sec:view} and~\ref{sec:view_impl}) of the object space, which formalizes the notion of ephemeral context within our
model by allowing programs to map objects for access,
and \emph{security contexts} (Section~\ref{sec:sec}) which define a thread's access rights to objects in the system.
\Twizzler provides invariant pointers (Chapter~\ref{ch:invariant}) in a low-coordination global address space (Chapter~\ref{ch:global}) for
programs, as well as primitives to ensure crash-consistency
(Section~\ref{sec:crash}). The thread abstraction is similar to modern operating systems; the
kernel provides scheduling, synchronization, and management primitives.
Figure~\ref{fig:twz_sys_overview} shows an overview of the system
organization and how different parts of the system operate on data objects.

\begin{SCfigure}[b]
    \centering
    \includegraphics[width=\linewidth]{fig/twz_sys_diag}
    \caption[Twizzler system overview]{\Twizzler system overview. Applications link to \texttt{musl} (a C library),
        \texttt{twix} (our Linux syscall emulation library), and \texttt{libtwz} (our standard
        library). Through \texttt{musl}, they may act on in-memory data with POSIX interfaces,
        though we expect \Twizzler applications to operate directly on in-memory data with
        the supplied Twizzler abstractions.}
    %They can directly access objects or they can use legacy POSIX methods.}
    %The \texttt{libtwz} library provides library-OS functionality and manages the
    %address space and kernel interaction. The kernel manages objects, schedules threads, and
    %enforces security by programming MMU hardware.}
    \label{fig:twz_sys_overview}
\end{SCfigure}


\Twizzler's kernel acts much like an
Exokernel~\cite{Kaashoek,engler:sosp95}, providing sufficient services for a
userspace library OS, called \emph{\libcore}, to provide an execution environment for applications.
The primary job of \libcore is to manage mappings of persistent objects into the address space (Section~\ref{sec:view})
and deal with invariant pointers.
\Twizzler also exposes a standard library that
provides higher level interfaces beyond raw access to memory. For example, software that better fits
message-passing semantics can use library routines that implement message-passing atop shared
memory. \Twizzler's standard library provides additional
higher level interfaces, including streams, logging, event notification, and many
others. Applications use these to easily build composable tools and pipelines for
operating on in-memory data structures without the performance loss and complexity of explicit I/O\@.


























\iffalse
    % TODO move this somewhere?
    \unedit{

        While considering a single machine as a distributed system\sidenote{While the system is
            distributed, and contains communicating entities, it is not \emph{truly} a traditional
            distributed system, since a single machine does not include partial-failure semantics (and the
            cases where it does are rare enough that they may be largely ignored); often a partial failure
            of a component is instantly upgrade to total failure. However, the messaging semantics of
            distributed systems are present.} has been examined from the
        perspective of OS design~\cite{baumann:sosp09}, the extent to which we are proposing has not been.
        \emph{Each} device, including CPU sockets, acts like a component containing a local memory with
        \emph{direct} access to a pool of shared, persistent storage.

        Such a model has two significant implications:
        \begin{enumerate}
            \item Protection of data within this system is of utmost importance, considering how each of
                  these devices
                  could run an entire operating system with application suites. The green lines in
                  Figure~\ref{fig:sys_arch} represent control paths, showing how memory access is administrated. Note
                  that the CPU, while similar in some respects to other devices, is distinguished in that it sets
                  security policies for the system\footnote{Only in supervisor mode; user-space applications can only
                      administrate devices that have been mapped into their address spaces.}. The \emph{enforcement}
                  of those policies is handled by the MMU and the IOMMU, both of which apply a mapping of virtual
                  addresses to physical addresses along with access restrictions.
            \item The addresses emitted by such devices are typically physical addresses that DMA engines
                  operate on. However, similar to \observation~\ref{hetero-mem} above, emitting physical
                  addresses has significant complications in our model, necessitating the need for hardware to
                  abstract physical memory into an object-space as well. Current systems can implement this
                  (along with protection) through the IOMMU for hardware devices and MMU for the CPU.
        \end{enumerate}

        These implications combined with \observation~\ref{hetero-mem} leads to another point:

        \observe{sls}{The CPU and devices must agree on a single, global abstracted view of
            shared physical memory, leading to a system-wide single-level store interface (shown in
            Figure~\ref{fig:log_sys_arch}). Protection is
            enforced by hardware devices such as the IOMMU, and devices can operate on data objects as
            opposed to physical memory directly.}

        In the case of specialized hardware running programs, this grants hardware better access to global
        memory because it does not need to coordinate and the system can trust it more (since the IOMMU can
        enforce security). For simple I/O controllers, like an NVMe device, it allows programming of such
        devices in a more efficient and simpler way---instead of translating object pages into physical
        addresses, it can issue DMA requests directly to object pages instead.


        \begin{SCfigure}
            \centering
            \includegraphics[width=\linewidth]{fig/log_sys_arch}
            \caption{Logical view of the system architecture. Mapping hardware such as the IOMMU would allow
                \emph{all} devices to see a single-level store instead of raw physical memory.}
            \label{fig:log_sys_arch}
        \end{SCfigure}


        An example of this would be if a NIC is processing data it received in a packet, and that data
        references data in another data object. If there were no abstraction layer, the NIC would need to
        either interrupt the CPU to ask where the data is, or it would need to keep a map of objects in
        physical memory itself. The latter adds to management costs of maintaining such maps for all
        hardware devices in the system when data placement changes, and the former results in performance
        degradation (due to interrupting the CPU) and requires additional management code to assist in
        making data transfers on behalf of the hardware devices\footnote{Many current devices today will
            still need this. However, we are planning for a system model in which it is less necessary. Many
            devices today fit this model, including some GPUs and special off-chip processing
            accelerators.}. This also leads to another \observation about data movement, since data
        processing decisions will need be made by devices autonomously:

        \observe{mem-to-mem}{Operating systems will need to provide a way for applications to specify data
            movement in a system declaratively, so that data movement decisions can be made largely without user
            involvement.}

        %A natural consequence of such a distributed system is heterogeneous computing, where we have
        %different types of processors accessing memory simultaneously. This could be as simple as an x86
        %processor and an ARM processor both operating on shared memory, where the system offloads
        %computationally nonintensive tasks to the ARM chip, but also encompasses our system design above.
        %For example, ARM GPUs can (and do) access global memory with cache coherence with the CPU (CITE).
        %The jury is still out on scaling-up cache coherence in this way, and systems will need to support
        %both devices that operate on only local memory and devices which operate on shared memory.



        The system design shown in
        Figure~\ref{fig:sys_arch} is not necessarily the end evolution of treating a single machine as a
        distributed system of components. We could imagine treating the CPU like any other component which
        accesses shared memory through a controller that itself enforces memory protections and mappings.
        However, such a model is easier to reach and program for as an evolution of our proposed system
        design presented here. Furthermore, Figure~\ref{fig:sys_arch} shows a system design that can be
        realized \emph{now}, allowing us to construct a real system to explore our goals.


    }


    \subsection{Hardware Versus Software}

    \unedit{
        The challenges introduced by BNVM in mapping virtual addresses to physical
        addresses are the different lifetimes of data with respect to
        the references to that data and the heterogeneity of physical memory. Software and hardware have
        different requirements for this mapping, in the following respects:

        \begin{enumerate}
            \item[Latency.] Prior persistent data access schemes relied on system calls to
                operate on persistent storage. BNVM's low-latency can no longer tolerate the cost of those
                system calls. Instead, programs must have direct access to
                persistent data via load/store instructions. Similarly, hardware must be able to
                transfer data to and from BNVM using DMA.
            \item[In-memory data structures.] Direct access to persistent memory removes the loading and
                unloading cost, but we must \emph{also} remove the cost of data serialization: since
                persistent data is located in-memory, programs can operate on data as in-memory data
                structures using standard programming techniques. This change
                is not an issue for hardware, which treats data as a ``bag of bits''.
            \item[Data lifetime.] If persistent data is stored as in-memory data structures, then
                applications need a way to refer to data such that references have the same lifetime
                as the referenced data, a requirement that fundamentally arises from the need for
                to construct references that encode the relationships between data. Applications can store
                \emph{persistent pointers} that encode a more persistent name for data than an ephemeral
                virtual address.
                Again, hardware has no such requirement, since it neither constructs
                nor interprets relationships between data. Devices that need to consider data
                relationships do so in software running atop the hardware.
            \item[Mappings.] Both software and hardware must have a way of translating a
                %$\langle \mathit{object}, \mathit{offset} \rangle$ reference
                persistent pointer into a physical address.
                This mapping may change frequently as the OS changes allocation of physical
                pages to data (\eg, to persist a piece of data).
                %to objects, potentially moving objects in and out of BNVM (\eg, to persist an object).
                Hardware need only know
                %the mappings for a short time, and need not even know the ``canonical''
                %name for the object---
                %it simply needs to know
                how to access data in memory
                for a single operation. In contrast, software must have longer-term mappings, and must
                be able to support data shared between threads, potentially mapped into the threads
                in different places.
            \item[Memory heterogeneity.] Hardware and the operating system must know about different
                types of memory, \eg DRAM and BNVM.
                Software need only know how to allocate
                from the different types, and should not deal with other differences. Hardware, however,
                will need to know when data is moved between memory types if it wishes to correctly emit loads and stores
                requested by software.
        \end{enumerate}

        Our design must support appropriate abstractions for both software and hardware, facilitating the
        use of \emph{persistent pointers} to long-lived data for applications while providing hardware and
        the OS with the ability to move data in, out of, and around physical memory, preferably only using
        existing hardware functionality. We first abstract memory into \emph{objects},
        where related data with similar access semantics (\eg, an entire B-tree, where all nodes are subject
        to the same access control, \emph{etc}.) are placed in a single object and identified by
        a \emph{globally} unique ID. IDs are formed via hashing, partitioned allocation, or other methods that prevent collisions
        of IDs across machines with high probability.
        %A programmer could, for example, place an entire B-tree within an object (as the
        %tree itself has similar access control, etc.).
        %, or create a tree that spans multiple objects with
        %pointers between them.

        We propose two abstractions to provide effective access to objects for both hardware and software:
        a \emph{global object space}, which facilitates persistent pointers (discussed below), and a
        \emph{logical object space}, which allows hardware to address data without needing knowledge of the
        data's physical location. We split these abstractions and consider them different because of the
        different requirements discussed above; software must worry about reference lifetime, whereas
        hardware's access to objects is disconnected from persistence. On the other hand, today's hardware
        must worry about physical location of data, which (with BNVM) is now more
        likely to change over time.
    }

    \unedit {
        \paragraph{Hardware and Memory}

        The interface presented to hardware must enable interaction with a
        \textit{heterogeneous memory system} and support the abstraction of an object space rather than a
        collection of flat memory spaces. Unlike applications, hardware has no need for
        creating long-lived pointers to locations within objects. Rather, hardware must
        have the ability to transfer large chunks of objects to, from, and around memory, and must be able to
        manage different \emph{types} of memory, preferably in a way that is largely transparent to
        applications.

        Requiring hardware to access a global object space through a $\langle \mathit{object},
            \mathit{offset} \rangle$ tuple is unnecessary overhead and complexity---hardware need only ever
        access the ``working set'' of objects currently undergoing computation. Thus, our design abstracts
        the view from hardware of physical memory into a \emph{logical object space}, an address space that
        maps contiguous objects within it to physical memory pages, managed by the
        operating system. This solves the heterogeneity problem by allowing hardware to refer to data within
        objects instead of physical addresses, whose lifetimes are much smaller than the objects, and
        doesn't require hardware to emit (likely large) addresses for the global address space.

        While SASOSes are not a viable solution to the problem of persistent pointers, they are
        \emph{a} solution to implementing the logical object space.  Hardware, and the CPU, are
        directly connected, reducing the cost of invalidation and coordination of an address space.
        Additionally, this address space is \emph{intermediate} and hidden from programs---a virtual
        address is translated to the logical object space, after which the address is translated to a
        physical location (shown in Figure~\ref{fig:twolevel}). This translation for
        applications (after the virtual address translation) and that of hardware is the same, reducing the
        management complexity for the OS, discussed in Section~\ref{sec:os:addrspace}.

    }

\fi

\section{A Historical Look}
\label{sec:historical}




%The introduction of byte-addressable non-volatile memory presents an outstanding
%opportunity to rethink the way that OS's handle memory and storage.

% Working with new technologies often allows past, merely theoretical ideas a
% chance to be practical.

\Twizzler's design
is shaped by fundamental OS
research~\cite{corbato_introduction_1965,chase:tocs94,k42,engler:sosp95,Kaashoek,engler1995avm,engler1995exterminate},
which,
while approaching similar topics as we described
previously, often did not consider all design
requirements we discussed simultaneously, nor did they have a view of hardware trends that we do today, resulting in an incomplete picture.
Recent research on building in-memory persistent data structures~\cite{volos:asplos11,coburn:asplos11,condit:sosp09,debnath:vldb10,lu:tos16,hu:atc17},
often focuses on building data structures that
provide failure atomicity and consistency for only persistence.
In contrast, we explore how hardware trends affect programming models and OS abstractions on the whole.
We draw from recent work on providing OS support for \NVM systems~\cite{caulfield:micro10}, as these align well with
in-memory sharing data structures, and work
providing recommendations for \NVM systems~\cite{mehra:ipdps04}, integrating
object-oriented techniques and simplified kernel design
to provide high-performance OS support for applications running on a single-level
store~\cite{shapiro:sosp99,bailey:hotos11}.

% Our approach will leverage non-volatile memory characteristics to greatly
% reduce the need for an OS stack and provide full system
% resumability.
%, we draw on prior object-oriented OS design work to
%simplify the kernel using techniques developed in systems such as K42~\cite{k42}
%and
%Exokernel~\cite{kaashoek_application_1997, engler:sosp95}.

%\subsection{Memory Model}
\paragraph{Memory and Object Model}

Multics was one of the first systems to use segments to partition memory and
support relocation~\cite{bensoussan:sosp69,daley:cacm68}. It used segments to
support location independence, but still stored them in a file system, requiring manual linkage
rather than the automated linkage in \Twizzler. Nonetheless, Multics demonstrated that the use of
segmenting for memory management can be a viable approach, though its
symbolic addresses were slow.

The core of \Twizzler's object space design uses concepts
from Opal~\cite{chase:tocs94}, which used a single virtual
address space for all processes on a system, making it easier to share
data between programs. However, Opal was a single-address space OS, which is insufficient for
a full data-centric system\sidenote{As we will soon see.},
and, while it resulted in a speedup of data transfer and sharing as well as interfacing with
devices, it did not address issues of file storage and name resolution. It also
still required a file
system, since there was no way to have a pointer refer to an object with changing identity,
whereas our approach
of late-binding for pointers
removes the need for an explicit file system.  Other single-address space
OSes, such as Mungi~\cite{heiser:scse9314},
Nemesis~\cite{roscoe:osr94}, and Sombrero~\cite{skousen:ipccc99}, show that
single address spaces have merit, but, like Opal, do not fully align with software and hardware trends today; in particular, how the use of fixed addresses
results in a great deal of coordination that is unnecessary in our approach.
OSes such as HYDRA~\cite{wulf:cacm74} provide functionality similar to invariant pointers;
however, in \Twizzler, we extend their use from procedures-referencing-data to
a more general approach. Furthermore, they required
heavy kernel involvement, an approach incompatible with
our design goals.

Single-level stores~\cite{shapiro:usenix02,shekita:uwtr956,dearle:cs94} remove the
memory versus persistent storage distinction, using a single model for data
at all levels. While well-known,
``little has appeared about them in the public
literature''~\cite{shapiro:usenix02}, even since the EROS paper.
Our work is partially inspired by Grasshopper~\cite{dearle:cs94}, AS/400, and orthogonal persistence systems,
but while these are designed to provide an illusion of persistent
memory, \Twizzler is built for both real \NVM and distributed sharing, and focuses on providing a truly global object space with
global references without cross-machine coordination.
Clouds~\cite{dasgupta:computer91} implemented a distributed object store in
which objects contained code, persistent data, and both volatile and
persistent heaps. Our approach uses lighter-weight objects, allowing direct
access to objects from outside, unlike Clouds. Software persistent
memory~\cite{guerra:atc12}, designed to operate within the constraints
of existing systems, built a persistent pointer system using explicit
serialization without cross-object references, in contrast to \Twizzler.

Recently, several projects have considered the impact of non-volatile
memories on OS structure. Bailey,
\etal~\cite{bailey:hotos11} suggest a single-level store design.
Faraboschi, \etal~\cite{faraboschi:hotos15} discuss challenges and inevitable system organization
arising from large \NVM, and we follow many of their recommendations.
%and using
%\NVM to ship a program as a checkpoint of a running process. 
The Moneta
project~\cite{caulfield:micro10} noted that removing the
heavyweight OS stack dramatically improved performance.
While Moneta focused on I/O performance, not on rethinking the
system stack, we leverage their approach to reduce OS
overhead as much as possible, even when the OS must intervene.
Lee and Won~\cite{lee:hpcc13} considered the impact of \NVM on
system initialization by addressing the issue of system boot as a way to restore
the system to a known state; we may need to include similar techniques to
address the problem of system corruption.

\paragraph{Object Model}
IBM's K42~\cite{k42}
inspired the high level design of \Twizzler. The
object-oriented approach to designing a micro or exokernel used in K42 is an
efficient design for implementing modular OS components.
Like K42, \Twizzler lazily maps in only the resources that an
application \emph{needs} to execute. Similar techniques for faulting-in objects at
run-time have been studied~\cite{Hosking1993}. Communication between objects in
\Twizzler is, in part, implemented as protected calls, similar to K42.

Emerald~\cite{jul_implementation_1991,jul:tocs88} and Mesos~\cite{Hindman}
implemented networked object mobility, which
we can also support. Emerald implemented a kernel, language, and
compiler to allow objects mobility using
wrapper data structures to track metadata and presenting
objects in an object-oriented language, impacting performance via added indirection for even simple
operations.

The \Twizzler object model was shaped by
NV-heaps~\cite{coburn:asplos11}, which provides memory-safe persistent objects
suitable for \NVM and describes safety pitfalls in
providing direct access to \NVM. While they
have language primitives to enable persistent
structures, \Twizzler provides a lower-level and uninhibited view of
objects like Mnemosyne~\cite{volos:asplos11}, allowing
more powerful programs to be built. Languages and libraries may impose
further restrictions on \NVM use, but \Twizzler itself does not.
Furthermore, \Twizzler's cross-object pointers allow external data
references by code, whereas NV-heap's and DSPM's~\cite{shan:socc17} pointers are
only internal. Existing work beyond Multics on external references shows and
recommends hardware support~\cite{wang:micro17,libpmem}, but provides a
static or per-process view of objects, unlike \Twizzler, limiting scalability and flexibility.
%and is not used in the context of OS software or
%libraries.

Projects such as PMFS~\cite{dulloor:eurosys14} and
NOVA~\cite{Xu:nova} provide a file system for \NVM. \Twizzler, in
contrast, provides direct \NVM access atop of a key-value interface of objects.
Although \Twizzler does not supply a file system, one can be built
atop it. While NOVA
and PMFS provide direct access to \NVM, NOVA adds indirection
with copies. Both use \texttt{mmap} (which falls short as
discussed above) and, unlike \Twizzler, require significant kernel interaction
when using persistent memory.

Our kernel that ``gets out of the way'' is influenced by systems
such as Exokernel~\cite{engler:sosp95} and SPIN~\cite{bershad:sosp95}, both of
which drew on Mach~\cite{accetta:usenix86s}. In
Exokernel, much of the OS is implemented in userspace, with the kernel providing only resource protection. Our approach is
similar in some respects, but goes further in providing a single unified
namespace for all objects, making it simpler to develop programs
that can leverage shared, in-memory state that can nonetheless persist.
In contrast, SPIN used type-safe languages to provide protection and
extensibility; our approach cannot rely upon language-provided type safety since
we want to provide a general purpose platform.

\subsection{Classifying Operating Systems}
Broadly speaking, operating systems can be classified several ways---how they handle and abstract
persistence, how they handle inter-process communication, how they access data objects, and how they
protect data and processes. We will be looking
primarily at single-level stores, since such an interface is a natural fit for in-memory data structures.

\subsubsection{Single Level Stores and Single Address Space Operating Systems}
\label{sec:sasos}

Closer persistence, and \NVM in particular, allows the implementation of a \textit{true}
single-level store, as has been suggested before~\cite{bailey:hotos11}.
Classic single-level store systems, such as AS/400, Cricket~\cite{shekita:uwtr956},
Grasshopper~\cite{dearli:cs94}, and
EROS~\cite{shapiro:usenix02}, hide the traditional two-level storage hierarchy of DRAM and disk
behind the illusion that all data is in memory. Since these systems present merely the illusion of persistence
through memory, they can be broken up into how they provide that illusion. AS/400, while presenting
a single-level store interface for data access and manipulation, requires explicit OS calls to
ensure persistence of data, while the others provide implicit persistence~\cite{dearli:cs94}.
Hardware trends indicate that explicit OS calls to persist data are unacceptable
due to their latency. The other systems follow different strategies for implementing implicit
persistence, including checkpoints to disk in EROS and completely invisible persistence in
Grasshopper. Both of these approaches are inappropriate for a data-centric system, since consistency must be more
fine-grained than checkpoints, and require \emph{some} application involvement.

Single address space operating systems (SASOSs) and single-level stores are fundamentally entwined,
where single-level stores are made easier to implement in a SASOS style, and SASOSs typically
present a single-level store interface. Opal~\cite{chase:sosp01}, Mungi~\cite{heiser:scse9314}, and
Sombrero~\cite{miller:osr00} are built for large virtual address spaces, and tie persistent objects
to virtual addresses for the duration of their lifetimes. This approach has merit for implementing
invariant pointers, but falls short in some respects. Firstly, modern
CPU address spaces are not large enough to address all of the data as object storage grows and
scales beyond a single machine without increasing pointer size and page-table depth, both of which
we would like to avoid. Secondly, the management cost of persisting the virtual mappings
outweigh the benefits; since pointers are stored directly, changing the address space (and
therefore updating all pointers associated with deleted or moved objects) becomes intractable.
Opal, in addition, still required a filesystem, which is a needless layer of abstraction handled
within the operating system. Other
SASOSs~\cite{roscoe:osr94,heiser:scse9314,miller:osr00} also do not take into account \NVM, and so
do not address the issues described above.
HYDRA~\cite{wulf:cacm74} also provides similar location independent pointer references, but these
were used primarily for procedures referencing data, not references to persistent or shared data, and required
heavy kernel involvement.

\subsubsection{Micro, Exo, Multi, oh my!}

Fundamental research into kernel design plays a large role in our design decisions. The three kernel
design philosophies we will discuss are Microkernel~\cite{accetta:usenix86s},
Exokernel~\cite{engler:sosp95}, and Multikernel~\cite{baumann:sosp09}:
\begin{itemize}
    \item The \emph{Microkernel} approach limits the responsibilities of the kernel to providing
          thread scheduling, process separation, and message passing primitives. It allows userspace
          servers to implement most of the OS functionality applications require. While this is
          advantageous in moving kernel functionality into userspace, it often does so by
          message-passing. We have the ability to do IPC on persistent objects through
          shared memory, so our systems should rely on this approach instead. Additionally, message
          passing in microkernels typically involves numerous kernel invocations, which hamper
          persistent and distributed data access.

    \item \emph{Exokernels} avoid as much operating system abstraction as possible, choosing instead
          only handle the bare minimum responsibilities needed for securely multiplexing in-kernel.
          Exokernels typically involve a user-space \textit{library operating system} (libos) that
          provide other functionality. Unlike the microkernel approach, these often involve
          procedure-based IPC~\cite{lauer:osr79,engler:sosp95} and present raw \emph{physical}
          resources to userspace instead of virtualized ones. While the reduced level of message
          passing is advantageous in our expected architecture, the direct presentation of physical resources
          to applications presents an unnecessary complexity for persistent data access, and the lack
          of abstraction for hardware is similarly costly for security and ease of system programming.

    \item The \emph{Multikernel} approach~\cite{baumann:sosp09} considers each NUMA domain or each individual core as a
          separate system running a full kernel, with little shared state between instances. While
          this approach has merit for the distributed nature of a single machine, it does not consider
          how shared-memory access to persistent memory could improve application and system design
          and performance. We plan to take this approach into consideration in our design.
          Furthermore, our above observations suggest that the multikernel approach does not go far
          enough---\emph{every} programmable device in this system needs to be treated as a separate
          system instance. Finally, multikernels do not address making hardware see a uniform
          object space, nor do they address the problem of invariant pointers in a global address space.
\end{itemize}

\subsubsection{Access Control and Capability Systems}
Our discussion thus far has not fully addressed the issue of \emph{data protection}. In a world
where hardware devices are free to act on shared global memory with the autonomy that we predict
here, protecting data is a significant issue from the perspective of operating system design. So far, the hardware
solutions available to us for implementing access to shared memory enable
protection as well though the IOMMU, whose original design was to both protect memory from hardware
and enable full device virtualization~\cite{markuze2016true}. We can leverage this hardware to apply
security enforcement as well as a coordinated mapping of objects within a fault zone.

While the enforcement is done through hardware, not the operating system\sidenote{A necessary consequence of
    removing the kernel from the data path!}, we can still separate mechanism and policy. The users
and system make the policy, while the operating system interprets the policy, verifies it against a
set of requirements and properties, and then programs the hardware to enforce the guarantees. This
means the kernel must have a way of verifying access control to an object during a fault. We look to
capability systems for solutions due to their common use in single-level stores and SASOSs, and
because they provide better least-access properties~\cite{capmyth}, which is important when hardware devices and
applications can cause irreparable damage to persistent data with normal memory accesses.


\begin{chconc}
    The data-centric approach has the potential to improve performance, simplicity, and efficiency. The design space we
    described in this chapter is large, but at its core, it has a simple idea: data is the center of programming, and
    placing data in a global address space can alleviate the context problem. While prior OS research has approached
    some of these ideas, rarely are they all combined and viewed alongside with our analysis of modern hardware trends.
    Now that we have gotten the trends, motivation, and backstory out of the way, the next few chapters will focus on
    design and implementation details for Twizzler.
\end{chconc}