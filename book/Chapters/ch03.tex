
\chapter{The Demands of Software}\label{ch:softwaredemands}

\section{The Old Ways}



\unedit{
    %will need to fix the R1 and R2 here, and this might involve moving around stuff in this block
    Current OS techniques do not meet requirements R1 and R2 as we set out above---file
    \texttt{read} and \texttt{write} interfaces,
    designed for sequential media and later expanded for block-based media,
    require significant kernel involvement and serialization, violating both requirements.
    %, and serialization that requires
    %the programmer to add complexity and overhead through multiple data representations and transformations.
    While support for these interfaces can be useful for legacy applications, as we will demonstrate,
    providing the programmer with abstractions designed \emph{for} \NVM both reduces complexity and
    improves performance.

    The \texttt{mmap} system call attempts to hide storage behind a memory interface through hidden
    data copies. But, with \NVM, these copies are wasteful, and \texttt{mmap} still has significant kernel
    involvement and the need for explicit \texttt{msync} calls. ``Direct Access'' (DAX)
    tries to retrofit \texttt{mmap} for \NVM by removing the redundant copy, but this \emph{still}
    fails to address requirement R2! Operating on persistent data through \texttt{mmap} requires the programmer to
    use either fixed virtual addresses, which presents an infeasible coordination problem
    as we scale across machines, or virtual addresses directly, which are ephemeral and require the
    context of the process that created them.



    Attempting to shoehorn \NVM programming atop POSIX interfaces (including \texttt{mmap}) results in
    complexity that arises from combining multiple partial solutions. Given some feature desired by an
    application, the \NVM framework can provide an integrated solution that meshes
    well with the existing support for persistent data structure manipulation and access, or it can
    fall-back to POSIX resulting in the programmer needing to understand two different ``feature
    namespaces'' and their interactions. An example of this is naming, where a programmer may need
    to turn to the filesystem to manage names in a completely orthogonal way to how the \NVM frameworks handles
    data references. For example, PMDK, a \NVM programming library, relies on a filesystem for naming and initial access to
    persistent memory objects, resulting in different kinds of references, feature sets from filesystems
    being applied (like security) while others are not (data access), and the complexity of
    understanding how the PMDK abstractions interact with the POSIX ones. Instead, our model prefers to
    build legacy support atop new abstractions (\S~\ref{sec:fbsdimpl}), and avoid falling back to legacy
    models for persistent data access. We will discuss another example,
    security, in our case study (\S~\ref{sec:eval}).


    Additionally, models that layer \NVM programming atop existing interfaces often fail to facilitate effective persistent data sharing and
    protection.  PMDK, for example, makes design choices that limit
    scalability, since its
    data objects are not self-contained and do not have a large enough ID space, resulting
    in the need to coordinate object IDs across machines~\cite{bittman:plos19}. For the same reason,
    although single-address space OSes~\cite{chase:tocs94} somewhat address requirement R1, they do
    not consider both requirements at once, nor do they provide an effective and scalable solution to
    long-term data references due to that same coordination complexity~\cite{bittman:hotstorage19}.


}


\unedit{
    The lifetime of data is, in general, longer than that of individual actors that access
    or manipulate it.
    On a single node, persistent data by definition outlives processes that operate on it.
    Similarly, in distributed systems, data outlives nodes that access or store it.
    Like processes, nodes may come and go in a distributed system.

    However, data access is often locked behind context that is tied to these shorter-lifetime actors, leading to unnecessary indirections.
    This is illustrated in how data references are typically handled, either:
    %and stored in one of three
    %ways:
    \begin{enumerate}
        \item \textbf{Explicit, context-sensitive.} In virtual address pointers,
              references between data are encoded explicitly but rely on context provided by
              the ephemeral process abstraction. These references cannot be reliably shared between
              applications and across nodes which do not have the context necessary to interpret them.
        \item \textbf{Implicit.} Many data relationships are implicit in applications.
              Although there is a relationship between \texttt{/etc/shadow} and \texttt{/etc/passwd},
              it is not explicit. This limits interoperability between programs, prevents
              relationship discovery, and results in a brittle environment if these files are
              moved or replaced.
        \item \textbf{Explicit, via mediators.} Relationships can be encoded explicitly without
              ephemera if we use a global name resolution service. For example, an HTML
              file can refer to a style sheet by name. However, this presupposes the existence (and
              availability) of a global mediator and restricts programs from agreeing on the identity
              of data behind a reference without complex inter-networking and expensive global coordination.
    \end{enumerate}

    In our view, the complexity of these mechanisms is a symptom of a more fundamental problem: access to long-lived data
    is unnecessarily mediated via ephemeral actors.
    We advocate a violent break from this actor-centric model of data access, in favor of a model that elevates \emph{data}
    as the systems' first-class citizen.  Doing so will require \emph{explicit
        context-free} references via globally unique identifiers (GUIDs) that name data objects. These
    references are independent of context (\eg process) that operates on them, and require that all context necessary to interpret references
    be stored within the data itself. Of course, we still need some understanding of
    context-sensitive relationships, for example late-binding of names. We do this via a
    ``two-level'' naming system in which GUIDs are \emph{authoritative} names to which we can bind
    additional names for purposes like local references, changing data identification, and discovery
    (see Sections \ref{sec:implementation} and \ref{sec:latebinding}).

    %\footnote{Note that, while our model is based on these references,
    %	there is room for context-sensitive data relationships in the form of late-binding, which we
    %	will discuss further in Section~\ref{sec:latebinding}.}
    %Such references require that all context necessary to interpret the data must
    %be stored within the data itself. 

    Viewing the past through the lens of the present, it was \emph{always} a mistake to entangle the context required to access
    and manipulate data with ephemeral actors.  However, in the days of slow networks, spinning disks and small memories,
    it was possible to hide these complexities and inefficiencies.  For one thing, disk-based I/O led to a model of sharing in
    which long-lived data and computable data were stored in different \emph{kinds} of memory, in different formats (due to serialization),
    with different reference formats (e.g., file names vs. virtual memory pointers).
    %% TODO: the following is candidate for removal if necessary
    The filesystem was a natural location to place the various hacks that were required to paper over the reality
    that interacting with long-lived data required figuring out how to name the short-lived processes that were the primary
    citizens of the operating system.

    Needless to say, a disaggregated system model---one in which a particular data reference may
    ultimately point to data on a different node from the one observing the reference---exacerbates all of these problems!
    Two different nodes that observe the same data reference should agree on what data is
    being referred to, and by the same token, an individual node observing two different data references
    should be able to determine if they point to the same data. This asks the question of \emph{how}
    to encode these explicit context-free data references while remaining efficient and avoiding global
    coordination.

}


\section{Serialization}

\section{Remote Procedure Calls}


\unedit{
    \subsection{Motivating Example}
    \label{sec:example}

    To illustrate the poor fit of RPC as a decoupling mechanism for some classes of applications,
    consider an example from the distributed inference problem for edge devices. Here,
    sensors in mobile devices with modest processing and storage resources (\eg mobile phones
    or autonomous vehicles) are the source of observations used both for training and inference.
    Recent work has focused on decoupling and distributing machine learning training across edge
    and cloud resources to minimize client-perceived latency, provide privacy guarantees, and
    maximize server-side throughput~\cite{federatedml,singh2019detailed}.

    In this example, we focus on the inference problem that arises in response to device input.
    Ideally, small models trained in the
    cloud (via a methodology such as federated learning) are periodically shipped in their entirety to
    edge devices, which perform local inference.  Several trends are upsetting this model. The first
    is the aggressive growth---roughly 10$\times$/year---of models, in particular language models.
    In 2018, the largest machine translation models at Google were 8.3 billion
    parameters~\cite{megatron}; a mere two years later, the largest models exceeded 800 billion parameters!
    Inference on sparse giant models which far exceed device resources must be performed server
    side, where model serving presents a substantial throughput bottleneck. This is further
    compounded by last-mile model customization for end users, in which inference tasks
    for different devices \emph{must} be performed on slightly different models.  As much as 70\%
    of the processing time~\cite{trims} for these model-serving applications is spent deserializing
    and loading the sparse personalized models into main memory at request time. Finally, users prefer
    local models remain local due to confidentiality concerns.

    Consider a concrete example that is bedeviled by all of these complexities at once. A mobile device,
    Alice, in possession of a locally-trained model and an activation, wishes to perform a
    classification task that requires a partition of a sparse global model, located on cloud resource
    Bob. Further, imagine that Alice cannot perform the inference locally, either because the global
    model fragment is too large or because she has inadequate local compute. Finally, imagine that Bob
    is overloaded, while a separate cloud resource, Carol, is mostly idle.
    Figure~\ref{fig:rpccopy}, also discussed in Section~\ref{sec:design}, shows this.

    An ideal solution will minimize the latency Alice perceives and maximize the throughput offered
    by both Bob and Carol, all while satisfying capacity constraints of each. It is easy to see
    that while this application requires decoupling, RPC is the wrong abstraction in terms of performance,
    expressivity, and flexibility.  Data movement---whether from storage to DRAM on Bob or from Bob
    to Carol---requires costly serialization. If moving the data to Carol and performing
    inference there is the optimal solution, the application logic on Alice must orchestrate this
    infrastructure-level concern, either by pushing the data through Alice (Figure~\ref{fig:rpccopy}.1)
    or having the RPC executed
    on Carol address Bob directly and pull (Figure~\ref{fig:rpccopy}.2), adding complexity. Further,
    heterogeneity among end devices makes the ``hard-coded'' data movement strategy
    brittle:
    a subsequent classification request from client device Dave will be forced to run inference
    on the server side even if it is equipped with the resources to do the work locally.

    A mechanism like RPC, in which movement of computation is explicit and movement of data implicit and
    limited, is a poor fit for any such use case, which we will generalize in Section~\ref{sec:design}.
    We need a flexible mechanism in which both code and data are mobile, but in which the
    application programmer need not make the movement of \emph{either} explicit.



    \paragraph*{Patterns of RPC.}
    %
    While RPC is a poor fit here, there are applications for which RPC provides adequate decoupling.
    RPC shines in situations where decoupling in the application meshes well with having little data
    movement, where an RPC endpoint either fronts large data, large compute relative to the invoker, or
    some combination, with \emph{small} arguments and return \emph{values}.
    But call-by-small-value is a significant constraint, and there are many
    classes of applications that do not fit.
    We cannot paper over this problem, either.
    Because
    RPC is disconnected from a global notion of data identity, we either \emph{cannot} do
    call-by-reference (because references must cross machine boundaries) or we must shoehorn this
    functionality into the application logic and the RPC's APIs, resulting in brittle, repetitive,
    complex code to deal with the coordination, caching, and prefetching that
    comes from distributed data and global references when data moves.








}